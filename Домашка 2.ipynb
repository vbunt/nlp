{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лера Бунтякова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложные слова для разбора:\n",
    "- слова с дефисами: мышь-соня, диван-кровать, Тянь-Шань, кресло-качалка (в итоге я вручную их склеивала по дефису, потому что задание было не про токенизацию, а про определение частей речи)\n",
    "- аббревиатуры: МФТИ, ВГИК (аббревиатуры можно принять за глаголы)\n",
    "- омонимы: вина (конечно, надо было взять омоним, у которого в разборе будут разные части речи)\n",
    "- имена собственные: Лидделл, Волков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOUN - существительные\n",
    "- PRON - местоимения \n",
    "- VERB - все формы глагола \n",
    "- ADVB - наречия и предикативы\n",
    "- ADJ - все прилагательные \n",
    "- CONJ - союзы \n",
    "- PRCL - частицы \n",
    "- COMP - сравнительные степени наречий и прилагательных \n",
    "- PRTF - полные причастия \n",
    "- PRTS - краткие причастия\n",
    "- GRND - деепричастия\n",
    "- NUMR - числительные\n",
    "- PREP - предлоги\n",
    "\n",
    "Я взяла сокращенный тегсет pymorphy с надеждой, что он будет универсальным. Он не был универсальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pandas.read_csv('alice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('alice.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(text): # так можно сделать, потому что я точно знаю, что других токенов там нет\n",
    "    tokens = re.findall('[а-яА-Я]+-*[а-яА-Я]*', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenisation(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_pymorphy(pos):\n",
    "    if pos == 'ADJF' or pos == 'ADJS':\n",
    "        return 'ADJ'\n",
    "    elif pos == 'INFN':\n",
    "        return 'VERB'\n",
    "    elif pos == 'PRED':\n",
    "        return 'ADVB'\n",
    "    else:\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8048780487804879\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "f = 0\n",
    "for index, row in true_df.iterrows():\n",
    "    token = row['token']\n",
    "    true_pos = row['pos']\n",
    "    pos = norm_pymorphy(morph.parse(token)[0].tag.POS)\n",
    "    if str(pos) == true_pos:\n",
    "        t += 1\n",
    "    else:\n",
    "        f += 1\n",
    "        \n",
    "print('accuracy: ', t/(t+f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_spacy(pos):\n",
    "    if pos == 'ADP':\n",
    "        return 'PREP'\n",
    "    elif pos == 'CCONJ' or pos == 'SCONJ':\n",
    "        return 'CONJ'\n",
    "    elif pos == 'PROPN':\n",
    "        return 'NOUN'\n",
    "    elif pos == 'DET':\n",
    "        return 'PRON'\n",
    "    elif pos == 'AUX':\n",
    "        return 'VERB'\n",
    "    elif pos == 'PART':\n",
    "        return 'PRCL'\n",
    "    elif pos == 'NUM':\n",
    "        return 'NUMR'\n",
    "    elif pos == 'ADV':\n",
    "        return 'ADVB'\n",
    "    else:\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8414634146341463\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "f = 0\n",
    "i = 0\n",
    "for index, row in true_df.iterrows():\n",
    "    token = row['token']\n",
    "    true_pos = row['pos']\n",
    "    \n",
    "    pos = doc[i].pos_\n",
    "    while pos in ['SPACE', 'PUNCT', ]:\n",
    "        i +=1\n",
    "        pos = doc[i].pos_\n",
    "    \n",
    "    text = doc[i].text\n",
    "    pos = norm_spacy(doc[i].pos_)\n",
    "    if doc[i+1].text == '-':\n",
    "        text = doc[i].text + '-' + doc[i+2].text\n",
    "        i += 2\n",
    "    i += 1\n",
    "    if str(pos) == true_pos:\n",
    "        t += 1\n",
    "    else:\n",
    "        f += 1\n",
    "        \n",
    "print('accuracy: ', t/(t+f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут мог быть мой код, но я уехала в горы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чанкер (с spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "не + глагол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не увидела\n",
      "не вижу\n",
      "не знала\n",
      "не переходить\n",
      "не нашелся\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc)):\n",
    "    if doc[i].text == 'не' and doc[i+1].pos_ == 'VERB':\n",
    "        print(doc[i].text, doc[i+1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "прилагательное + существительное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мартовский Заяц\n",
      "Бедная Мышь\n",
      "большое кресло\n",
      "Мартовский Заяц\n",
      "Мартовский Заяц\n",
      "Мартовский Заяц\n",
      "Мартовский Заяц\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc)):\n",
    "    if doc[i].pos_ == 'ADJ' and (doc[i+1].pos_ == 'NOUN' or doc[i+1].pos_ == 'PROPN'):\n",
    "        print(doc[i].text, doc[i+1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "наречие + глагол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бодро предложил\n",
      "здесь нет\n",
      "слишком обросла\n",
      "широко открыл\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc)):\n",
    "    if doc[i].pos_ == 'ADV' and doc[i+1].pos_ == 'VERB':\n",
    "        print(doc[i].text, doc[i+1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут тоже мог быть мой код, но я уехала в горы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
